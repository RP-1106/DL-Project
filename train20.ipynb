{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fbd4e7cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Rhea Pandita\\anaconda3\\lib\\site-packages\\tensorflow_addons\\utils\\tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.data import Dataset\n",
    "from ipynb.fs.full.model4 import Code2Model\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import MeanSquaredError\n",
    "from tensorflow.keras.metrics import Mean\n",
    "from torchvision import datasets, transforms\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "import numpy as np\n",
    "import random\n",
    "from tensorflow_addons.layers.normalizations import GroupNormalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab5fc908",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Mixed precision compatibility check (mixed_float16): WARNING\n",
      "The dtype policy mixed_float16 may run slowly because this machine does not have a GPU. Only Nvidia GPUs with compute capability of at least 7.0 run quickly with mixed_float16.\n",
      "If you will use compatible GPU(s) not attached to this host, e.g. by running a multi-worker model, you can ignore this warning. This message will only be logged once\n"
     ]
    }
   ],
   "source": [
    "checkpoint_callback = ModelCheckpoint(\"model_checkpoint.h5\", save_best_only=True)\n",
    "tf.keras.mixed_precision.set_global_policy('mixed_float16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7c73598b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the number of training and testing samples you want to use\n",
    "desired_train_size = 20000\n",
    "desired_test_size = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8e630a82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# Load the CIFAR-10 dataset\n",
    "train_dataset = datasets.CIFAR10(\n",
    "    root=\"data\", train=True, download=True, transform=transforms.Compose([\n",
    "        transforms.ToTensor()\n",
    "        #transforms.Resize((128, 128))  # Resize images to (128, 128)\n",
    "    ])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5fcdd75a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "test_dataset = datasets.CIFAR10(\n",
    "    root=\"data\", train=False, download=True, transform=transforms.Compose([\n",
    "        transforms.ToTensor()\n",
    "        #transforms.Resize((128, 128))  # Resize images to (128, 128)\n",
    "    ])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6cad945",
   "metadata": {},
   "source": [
    "print(train_dataset.shape)\n",
    "print(test_dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8628fbed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle and split the training dataset into training and testing sets\n",
    "train_size = desired_train_size\n",
    "test_size = desired_test_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3ff2c53e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create TensorFlow Dataset objects for training and testing\n",
    "batch_size = 8\n",
    "train_loader = Dataset.from_tensor_slices((train_dataset.data[:train_size], train_dataset.targets[:train_size])).batch(batch_size).shuffle(buffer_size=train_size)\n",
    "test_loader = Dataset.from_tensor_slices((test_dataset.data[:test_size], test_dataset.targets[:test_size])).batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "882e09dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <bound method UNetMidBlock2DCrossAttn.call of <ipynb.fs.full.model4.UNetMidBlock2DCrossAttn object at 0x0000022F2CF5B2E0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: '{' was never closed (<unknown>, line 1)\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method UNetMidBlock2DCrossAttn.call of <ipynb.fs.full.model4.UNetMidBlock2DCrossAttn object at 0x0000022F2CF5B2E0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: '{' was never closed (<unknown>, line 1)\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    }
   ],
   "source": [
    "input_layer = Input(shape=(32, 32, 3))\n",
    "output_layer = Code2Model(32, 32, 16)(input_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "409a1ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(inputs=input_layer, outputs=output_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "36e4aafe",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Adam(learning_rate=1e-4)\n",
    "criterion = MeanSquaredError()\n",
    "metric  = tf.keras.metrics.RootMeanSquaredError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8a4c68ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=optimizer, loss=criterion, metrics=[metric],sample_weight_mode=\"temporal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4a8f74b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "2500/2500 [==============================] - 9147s 4s/step - loss: nan - root_mean_squared_error: nan - val_loss: inf - val_root_mean_squared_error: 4.7711\n",
      "Epoch 2/5\n",
      "2500/2500 [==============================] - 8824s 4s/step - loss: nan - root_mean_squared_error: nan - val_loss: inf - val_root_mean_squared_error: 4.7567\n",
      "Epoch 3/5\n",
      "2500/2500 [==============================] - 8834s 4s/step - loss: nan - root_mean_squared_error: nan - val_loss: inf - val_root_mean_squared_error: 4.7410\n",
      "Epoch 4/5\n",
      "2500/2500 [==============================] - 8961s 4s/step - loss: nan - root_mean_squared_error: nan - val_loss: inf - val_root_mean_squared_error: 4.7272\n",
      "Epoch 5/5\n",
      "2500/2500 [==============================] - 8810s 4s/step - loss: nan - root_mean_squared_error: nan - val_loss: inf - val_root_mean_squared_error: 4.7156\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x22f2cf2ec80>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_loader, epochs=5, validation_data=test_loader,\n",
    "         callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8244247d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Rhea Pandita\\anaconda3\\lib\\site-packages\\keras\\src\\engine\\training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "# Save the model as a .pt file\n",
    "model.save(\"model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d264a5ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
